{"cells":[{"cell_type":"markdown","source":["# Imports"],"metadata":{"id":"Qo2PTYCMFNHW"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"r1SqxPqxLG2_"},"outputs":[],"source":["import numpy as np\n","#%matplotlib notebook\n","%matplotlib inline\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","\n","from IPython.core.debugger import set_trace\n","from tensorflow import keras\n","import warnings\n","warnings.filterwarnings('ignore')\n","from sklearn.model_selection import KFold"]},{"cell_type":"markdown","metadata":{"id":"nPNwId5c2VO_"},"source":["# Data Preprocessing"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"phWN5w-7GJOK"},"outputs":[],"source":["# packaging it all into a function\n","def preprocess_fashion_mnist():\n","  import random as rand\n","   \n","   \n","  (x_train, y_train), (x_test, y_test) = keras.datasets.fashion_mnist.load_data()\n","  \n","  mean_mat = np.mean(x_train, axis=0)\n","  # centering the data by removing the pixel wise mean from every pixel in every image\n","  x_train_centered = x_train - mean_mat\n","  x_test_centered = x_test - mean_mat\n","  # normalizing the grayscale values to values in interval [0,1]\n","  x_train_normalized = x_train_centered/255.0\n","  x_test_normalized = x_test_centered/255.0\n","\n","  #finally, flattening the data\n","  x_train = np.reshape(x_train_normalized, (60000,784))\n","  x_test = np.reshape(x_test_normalized, (10000, 784))\n","  #converting the test data to one hot encodings\n","  y_train = keras.utils.to_categorical(y_train, num_classes=10)\n","  y_test = keras.utils.to_categorical(y_test, num_classes=10)\n","  \n","  return x_train, y_train, x_test, y_test\n","# x_train, y_train, x_test, y_test = preprocess_fashion_mnist()"]},{"cell_type":"markdown","metadata":{"id":"viQL0Q9L2dAM"},"source":["# General Model Implementations\n"]},{"cell_type":"markdown","metadata":{"id":"oGwQgzDZ6XaO"},"source":["Activation functions"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dxQ35WhEQLdR"},"outputs":[],"source":["#activation functions\n","softmax1D = lambda z: np.exp(z)/float(sum(np.exp(z)))\n","softmax2D = lambda z: np.array([np.exp(i)/float(sum(np.exp(i))) for i in z])\n","\n","# Logistic\n","logistic = lambda z: 1./ (1 + np.exp(-z))\n","\n","# Tanh\n","tanh = lambda x: 2./ (1+np.exp(-2*x)) -1\n","tanh_grad = lambda x: 1 - np.square(2./ (1+np.exp(-2*x)) -1)\n","\n","# Leaky ReLu\n","def leaky_relu(x):\n","  alpha = 0.1\n","  x=np.array(x).astype(float)\n","  np.putmask(x, x<0, alpha*x)\n","  return x\n","\n","def leaky_relu_grad(x):\n","  alpha = 0.1\n","  x=np.array(x).astype(float)\n","  x[x>0]=1\n","  x[x<=0]=alpha\n","  return x\n","\n","  \n","# ReLu\n","def relu(x):\n","  x=np.array(x).astype(float)\n","  np.putmask(x, x<0, 0)\n","  return x\n","  \n","def relu_grad(x):\n","  x=np.array(x).astype(float)\n","  x[x>0]=1\n","  x[x<=0]=0\n","  return x"]},{"cell_type":"markdown","metadata":{"id":"h524tYb46mwq"},"source":["Accuracy function"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hgeOx3cLuwEl"},"outputs":[],"source":["def evaluate_acc(pred, truth):\n","  counter =0\n","  \n","  for i in range(len(pred)):\n","    maxVal = np.where(pred[i] == np.amax(pred[i]))\n","    counter += 1 if maxVal == np.where(truth[i]==1) else 0\n","  return counter * 100.0 / float(len(pred))\n","  "]},{"cell_type":"markdown","metadata":{"id":"p44VS-enzmOU"},"source":["# 1 Layer MLP"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HvtNhYiIzks4"},"outputs":[],"source":["class MLP1Layer:\n","    \n","    def __init__(self, M = 128, num_classes = 10):\n","        self.M = M\n","        self.num_classes = num_classes\n","        self.params = []\n","            \n","    def fit(self, x, y, optimizer):\n","        N,D = x.shape\n","        def gradient(x, y, params):\n","            \n","            # print('x=',x.shape)\n","            w = params[0] # v.shape = (D, M), w.shape = (M)\n","            z = np.dot(x, w)# print('z=',z.shape)\n","            yh = softmax2D(z)#N\n","            dy = yh - y #N\n","            \n","            dw = np.dot(x.T, dy)/N #M  \n","            dparams = [dw]\n","            return dparams\n","        \n","        # w = np.random.randn(self.M) * .01\n","        # v = np.random.randn(D,self.M) * .01\n","        initializer = keras.initializers.GlorotNormal()\n","        w = initializer(shape=(D, self.num_classes))\n","        # v = initializer(shape=(D, self.M))\n","        params0 = [w]\n","        self.params = params0\n","\n","        self.params = optimizer.run(gradient, x, y, params0) #optimizer.run_mini_batch(gradient, x, y, params0) #\n","        # print(self.params)\n","        return self\n","    \n","    def predict(self, x):\n","        w = self.params[0]\n","        yh = softmax2D(np.dot(x, w))#N\n","        return yh"]},{"cell_type":"markdown","metadata":{"id":"2sFWzkOs6pJ4"},"source":["# 2 Layer MLP"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"a9CPhDEuLG3C"},"outputs":[],"source":["class MLP2Layer:\n","    \n","    def __init__(self, M = 128, num_classes = 10):\n","        self.M = M\n","        self.num_classes = num_classes\n","            \n","    def fit(self, x, y, optimizer):\n","        N,D = x.shape\n","\n","        def gradient(x, y, params):\n","            v, w = params # v.shape = (D, M), w.shape = (M)\n","            q = np.dot(x, v)\n","            z = relu(q) #N x M   \n","            yh = softmax2D(np.dot(z, w))#N\n","            acc = evaluate_acc(yh,y)\n","            dy = yh - y #N\n","            dw = np.dot(z.T, dy)/N #M  \n","            dz = np.dot(dy, w.numpy().T) #N x M                   = (yh-y)*w from slide 16\n","            temp = dz * relu_grad(q) #(1024,128) (32,128)\n","            dv = np.dot(x.T, dz * relu_grad(q))/N #D x M   = (yh-y)*w*(activation)'*x\n","            dparams = [dv, dw]\n","            return dparams, acc\n","        \n","        # w = np.random.randn(self.M) * .01\n","        # v = np.random.randn(D,self.M) * .01\n","        initializer = keras.initializers.GlorotNormal()\n","        w = initializer(shape=(self.M, self.num_classes))\n","        v = initializer(shape=(D, self.M))\n","        params0 = [v,w]\n","        self.params, batch_acc = optimizer.run_mini_batch(gradient, x, y, params0, 64) #optimizer.run(gradient, x, y, params0) #\n","        return self, batch_acc\n","    \n","    def predict(self, x):\n","        v, w = self.params\n","        z = relu(np.dot(x, v)) #N x M\n","        yh = softmax2D(np.dot(z, w))#N\n","        return yh"]},{"cell_type":"markdown","metadata":{"id":"MQ1iAtHMzxRP"},"source":["# 3 Layer MLP"]},{"cell_type":"markdown","metadata":{"id":"7gNI0gF_9ANz"},"source":["### 3 Layer Relu"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FkaVF29IzzLB"},"outputs":[],"source":["class MLP3Layer:\n","    \n","    def __init__(self, M = 128, num_classes = 10):\n","        self.M = M\n","        self.num_classes = num_classes\n","            \n","    def fit(self, x, y, optimizer):\n","        N,D = x.shape\n","        def gradient(x, y, params):\n","            v1, v2, w = params # v1.shape = (D, M), v2.shape = (M, M) w.shape = (M)\n","            q1 = np.dot(x, v1) \n","            z1 = relu(q1) #N x M\n","            q2 = np.dot(z1, v2) # N\n","            z2 = relu(q2)\n","            yh = softmax2D(np.dot(z2, w))#N\n","            ## Backpropagation\n","            ## 1st layer\n","            dy = yh - y #N\n","            dw = np.dot(z2.T, dy)/N #M  \n","            ## 2nd Layer            \n","            dz2 = np.dot(dy, w.numpy().T) #N x M                   = (yh-y)*w from slide 16\n","            dv2 = np.dot(z1.T, dz2 * relu_grad(q2))/N #D x M       = (yh-y)*w*(activation)'*x\n","            ## 3rd Layer\n","            dz1 = np.dot(dz2, v2.numpy().T) #N x M                 = (yh-y)*w from slide 16\n","            dv1 = np.dot(x.T, dz1 * relu_grad(q1))/N #D x M        = (yh-y)*w*(activation)'*x\n","            dparams = [dv1, dv2, dw]\n","            return dparams\n","        \n","        # w = np.random.randn(self.M) * .01\n","        # v = np.random.randn(D,self.M) * .01\n","        initializer = keras.initializers.GlorotNormal()\n","        w = initializer(shape=(self.M, self.num_classes))\n","        v2 = initializer(shape=(self.M, self.M))\n","        v1 = initializer(shape=(D, self.M))\n","        \n","        params0 = [v1, v2,w]\n","        self.params = optimizer.run_mini_batch(gradient, x, y, params0) #optimizer.run(gradient, x, y, params0)#\n","        return self\n","    \n","    def predict(self, x):\n","        v1, v2, w = self.params\n","        z1 = relu(np.dot(x, v1)) #N x M\n","        z2 = relu(np.dot(z1, v2))\n","        yh = softmax2D(np.dot(z2, w))#N\n","        return yh"]},{"cell_type":"markdown","metadata":{"id":"JuAx8-kYK5QR"},"source":["### 3 Layer MLP - Leaky ReLu"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"29yPBOV7K9P_"},"outputs":[],"source":["class MLPLeakyRelu:\n","    \n","    def __init__(self, M = 128, num_classes = 10):\n","        self.M = M\n","        self.num_classes = num_classes\n","        \n","            \n","    def fit(self, x, y, optimizer):\n","        N,D = x.shape\n","        def gradient(x, y, params):\n","            v1, v2, w = params # v1.shape = (D, M), v2.shape = (M, M) w.shape = (M)\n","            q1 = np.dot(x, v1) \n","            z1 = leaky_relu(q1) #N x M\n","            q2 = np.dot(z1, v2) # N\n","            z2 = leaky_relu(q2)\n","            yh = softmax2D(np.dot(z2, w))#N\n","            ## Backpropagation\n","            ## 1st layer\n","            dy = yh - y #N\n","            dw = np.dot(z2.T, dy)/N #M  \n","            ## 2nd Layer                  \n","            # dz = np.dot(dy.T, w)\n","            dz2 = np.dot(dy, w.numpy().T) #N x M                   = (yh-y)*w from slide 16\n","            dv2 = np.dot(z1.T, dz2 * leaky_relu_grad(q2))/N #D x M   = (yh-y)*w*(activation)'*x\n","            ## 3rd Layer\n","            dz1 = np.dot(dz2, v2.numpy().T) #N x M                   = (yh-y)*w from slide 16\n","            dv1 = np.dot(x.T, dz1 * leaky_relu_grad(q1))/N #D x M   = (yh-y)*w*(activation)'*x\n","            dparams = [dv1, dv2, dw]\n","            return dparams\n","        \n","         initializer = keras.initializers.GlorotNormal()\n","        w = initializer(shape=(self.M, self.num_classes))\n","        v2 = initializer(shape=(self.M, self.M))\n","        v1 = initializer(shape=(D, self.M))\n","        \n","        params0 = [v1, v2,w]\n","        self.params = optimizer.run_mini_batch(gradient, x, y, params0) #optimizer.run(gradient, x, y, params0)#\n","        return self\n","    \n","    def predict(self, x):\n","        v1, v2, w = self.params\n","        z1 = relu(np.dot(x, v1)) #N x M\n","        z2 = relu(np.dot(z1, v2))\n","        yh = softmax2D(np.dot(z2, w))#N\n","        return yh"]},{"cell_type":"markdown","metadata":{"id":"mZbtrTLdLBGY"},"source":["### 3 Layer MLP - TanH"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"echOzNTtLE42"},"outputs":[],"source":["class MLPTanh:\n","    \n","    def __init__(self, M = 128, num_classes = 10):\n","        self.M = M\n","        self.num_classes = num_classes\n","            \n","    def fit(self, x, y, optimizer):\n","        N,D = x.shape\n","        def gradient(x, y, params):\n","            v1, v2, w = params # v1.shape = (D, M), v2.shape = (M, M) w.shape = (M)\n","            q1 = np.dot(x, v1) \n","            z1 = tanh(q1) #N x M\n","            q2 = np.dot(z1, v2) # N\n","            z2 = tanh(q2)\n","            yh = softmax2D(np.dot(z2, w))#N\n","            ## Backpropagation\n","            \n","            ## 1st layer\n","            dy = yh - y #N\n","            dw = np.dot(z2.T, dy)/N #M  \n","            \n","            ## 2nd Layer                  \n","            dz2 = np.dot(dy, w.numpy().T) #N x M                   = (yh-y)*w from slide 16\n","            dv2 = np.dot(z1.T, dz2 * tanh_grad(q2))/N #D x M   = (yh-y)*w*(activation)'*x\n","            ## 3rd Layer\n","            dz1 = np.dot(dz2, v2.numpy().T) #N x M                   = (yh-y)*w from slide 16\n","            dv1 = np.dot(x.T, dz1 * tanh_grad(q1))/N #D x M   = (yh-y)*w*(activation)'*x\n","            dparams = [dv1, dv2, dw]\n","            return dparams\n","        \n","        initializer = keras.initializers.GlorotNormal()\n","        w = initializer(shape=(self.M, self.num_classes))\n","        v2 = initializer(shape=(self.M, self.M))\n","        v1 = initializer(shape=(D, self.M))\n","        \n","        params0 = [v1, v2,w]\n","        self.params = optimizer.run_mini_batch(gradient, x, y, params0) #optimizer.run(gradient, x, y, params0)#\n","        return self\n","    \n","    def predict(self, x):\n","        v1, v2, w = self.params\n","        z1 = relu(np.dot(x, v1)) #N x M\n","        z2 = relu(np.dot(z1, v2))\n","        yh = softmax2D(np.dot(z2, w))#N\n","        return yh"]},{"cell_type":"markdown","metadata":{"id":"sxK-YVliHv14"},"source":["# Dropout 3-layer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"q5wOKrK3H4cf"},"outputs":[],"source":["class MLPReluDropout:\n","    \n","    def __init__(self, M = 128, num_classes = 10):\n","        self.M = M\n","        self.num_classes = num_classes\n","        \n","            \n","    def fit(self, x, y, optimizer):\n","        N,D = x.shape\n","        def gradient(x, y, params):\n","            p2 = 0.5\n","            v1, v2, w = params # v1.shape = (D, M), v2.shape = (M, M) w.shape = (M)\n","            q1 = np.dot(x, v1)\n","            drop1 = np.random.binomial(1,p2,size=q1.shape[1])/p2 \n","            q1 *= drop1 \n","            z1 = relu(q1) #N x M\n","\n","            q2 = np.dot(z1, v2) # N\n","            q2 *= np.random.binomial(1,p2,size=q2.shape[1])/p2\n","            z2 = relu(q2)\n","            \n","            yh = softmax2D(np.dot(z2, w))#N\n","            train_acc = evaluate_acc(yh,y)\n","            ## Backpropagation    \n","            ## 1st layer\n","            dy = yh - y #N\n","            dw = np.dot(z2.T, dy)/N #M  \n","            \n","            ## 2nd Layer                  \n","            dz2 = np.dot(dy, w.numpy().T) #N x M                   = (yh-y)*w from slide 16\n","            dv2 = np.dot(z1.T, dz2 * relu_grad(q2))/N #D x M   = (yh-y)*w*(activation)'*x\n","            \n","            ## 3rd Layer\n","            dz1 = np.dot(dz2, v2.numpy().T) #N x M                   = (yh-y)*w from slide 16\n","            dv1 = np.dot(x.T, dz1 * relu_grad(q1))/N #D x M   = (yh-y)*w*(activation)'*x\n","            dparams = [dv1, dv2, dw]\n","            return dparams, train_acc\n","        \n","        initializer = keras.initializers.GlorotNormal()\n","        w = initializer(shape=(self.M, self.num_classes))\n","        v2 = initializer(shape=(self.M, self.M))\n","        v1 = initializer(shape=(D, self.M))\n","        \n","        params0 = [v1, v2,w]\n","        self.params, batch_train_accs = optimizer.run_mini_batch(gradient, x, y, params0) #optimizer.run(gradient, x, y, params0)#\n","        return self, batch_train_accs\n","    \n","    def predict(self, x):\n","        v1, v2, w = self.params\n","        z1 = relu(np.dot(x, v1)) #N x M\n","        z2 = relu(np.dot(z1, v2))\n","        yh = softmax2D(np.dot(z2, w))#N\n","        return yh"]},{"cell_type":"markdown","metadata":{"id":"o07x1kJe21uP"},"source":["# Batch Implementation"]},{"cell_type":"markdown","metadata":{"id":"WON0hiNiLG3E"},"source":["In the implementation above we have used a list data structure to maintain model parameters and their gradients. Below I have modified the `GradientDescent` class to also work with a list of parameters. One sournce of confusion in the above implementation is the gradient calculation. While in the slides during the lectures \n","we calculated the partial derivative for individual parameters, here, we use vector and matrix operations to calculate the derivative for *all* parameters. "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"edVLJC0tqSpA"},"outputs":[],"source":["def mini_batcher(x, y, mini_batch_size):\n","  zipped = np.hstack( (x, y ) )\n","  np.random.shuffle(zipped)\n","  x_batches, y_batches = [], []\n","  mini_batches = []\n","  batch_num = x.shape[0] // mini_batch_size \n","  for i in range(batch_num):\n","    x_batch = zipped[ i * mini_batch_size : (i+1) * mini_batch_size, :-10]\n","    y_batch = zipped[ i * mini_batch_size : (i+1) * mini_batch_size, -10:]\n","    mini_batches.append( ( x_batch, y_batch) )\n","  if x.shape[0] % mini_batch_size != 0:\n","    x_batch = zipped[ batch_num * mini_batch_size :, :-10]\n","    y_batch = zipped[ batch_num * mini_batch_size :, -10:]\n","    mini_batches.append( ( x_batch, y_batch ) )\n","  return mini_batches"]},{"cell_type":"markdown","metadata":{"id":"rt5K2LpN3KcI"},"source":["# Gradient Descent \n","\n","---\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LRgwO0YHLG3E"},"outputs":[],"source":["class GradientDescent:\n","    \n","    def __init__(self, learning_rate=.001, max_iters=800, epsilon=1e-8, batch_size=32):\n","        self.learning_rate = learning_rate\n","        self.max_iters = max_iters\n","        self.epsilon = epsilon\n","        \n","    def run(self, gradient_fn, x, y, params):\n","        norms = np.array([np.inf])\n","        t = 1\n","        while np.any(norms > self.epsilon) and t < self.max_iters:\n","            grad = gradient_fn(x, y, params)\n","            for p in range(len(params)):\n","                params[p] -= self.learning_rate * grad[p]\n","            t += 1\n","            norms = np.array([np.linalg.norm(g) for g in grad])\n","        print(t)\n","        return params\n","    \n","    def run_mini_batch(self, gradient_fn, x, y, params, batch_size=32):\n","        norms = np.array([np.inf])\n","        t=1\n","        temp_acc,batch_acc, chunk= [], [], []\n","        mini_batches = mini_batcher(x, y, batch_size)\n","        print(\"#mini batches\",len(mini_batches))\n","        batch_index = 0\n","\n","        while np.any(norms > self.epsilon) and t < self.max_iters*len(mini_batches):\n","            \n","            if(batch_index == batch_size):\n","              mini_batches = mini_batcher(x, y, batch_size)\n","              batch_index = 0\n","            batch_index +=1\n","\n","            x_temp, y_temp = mini_batches[t % ( len(mini_batches)-1 ) ][0], mini_batches[t % ( len(mini_batches)-1 ) ][1]\n","            grad, temp_acc = gradient_fn(x_temp, y_temp, params)\n","            chunk.append(temp_acc)\n","            if t % 10000 == 0:\n","              print(f\"Epoch{t}:{temp_acc}%\")\n","\n","            for p in range(len(params)):\n","                params[p] -= self.learning_rate * grad[p]\n","            if t%len(mini_batches) == 2:\n","              batch_acc.append(np.mean(chunk))\n","              chunk = []\n","            t += 1\n","            norms = np.array([np.linalg.norm(g) for g in grad])\n","        return params, batch_acc"]},{"cell_type":"markdown","metadata":{"id":"4KaoJ2CFI_MD"},"source":["# Hyperparameter Tuning"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"itObB3ykSetx"},"outputs":[],"source":["def hyper_tuning(model, activ, x_train, y_train):\n","  kf = KFold(5)\n","  acc_vals = []\n","  \n","  learning_rate = [0.001, 0.002, 0.004]\n","  batch_size = [16, 32, 64]\n","  for btch in batch_size:\n","    for lr in learning_rate:\n","      optimizer = GradientDescent(learning_rate = lr, batch_size=btch)\n","         \n","      avg_acc = 0;       \n","      for k, (train, test) in enumerate(kf.split(x_train, y_train)):\n","        \n","          temp_model = model(M=128)\n","          temp_model.fit(x_train[train], y_train[train], optimizer)\n","          y_test_pred = temp_model.predict(x_train[test])\n","          temp_acc = evaluate_acc(y_test_pred, y_train[test])\n","          avg_acc += temp_acc\n","      avg_acc = avg_acc/5\n","      acc_vals.append(avg_acc)\n","  data = {'learningRate' : [0.001, 0.002, 0.004, 0.001, 0.002, 0.004, 0.001, 0.002, 0.004], \n","          'batchSize':[16, 16, 16, 32, 32, 32, 64, 64, 64],\n","          'C':[100.0, 10.0, 1.0, .01, 100.0, 10.0, 1.0, .01,\n","               100.0, 10.0, 1.0, .01, 100.0, 10.0, 1.0, .01,\n","               100.0, 10.0, 1.0, .01, 100.0, 10.0, 1.0, .01,\n","               100.0, 10.0, 1.0, .01, 100.0, 10.0, 1.0, .01, 100.0, 10.0, 1.0, .01],\n","          'accuracies': acc_vals\n","          }\n","  acc = pd.DataFrame(data)\n","  print(acc)\n","  return acc"]},{"cell_type":"markdown","metadata":{"id":"Sx13m2QLRjZF"},"source":["# MNIST DataSet\n","---"]},{"cell_type":"markdown","metadata":{"id":"Yc0tkpgV0LrK"},"source":["## Model1 - No hidden layers"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"7jAnXJWrLG3M","outputId":"f268953f-4c79-47aa-b6f7-a5d9f10b0e1e"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch10000:6.25%\n","Epoch20000:9.375%\n","Epoch30000:6.25%\n","Epoch40000:9.375%\n","Epoch50000:6.25%\n","Epoch60000:15.625%\n","Epoch70000:15.625%\n","Epoch80000:12.5%\n","Epoch90000:9.375%\n","Epoch100000:28.125%\n","Epoch110000:15.625%\n","Epoch120000:25.0%\n","Epoch130000:25.0%\n","Epoch140000:15.625%\n","Epoch150000:28.125%\n","Epoch160000:21.875%\n","Epoch170000:40.625%\n","Epoch180000:25.0%\n","Epoch190000:18.75%\n","Epoch200000:21.875%\n","Epoch210000:28.125%\n","Epoch220000:21.875%\n","Epoch230000:21.875%\n","Epoch240000:31.25%\n","Epoch250000:25.0%\n"]}],"source":["x_train, y_train, x_test, y_test = preprocess_fashion_mnist()\n","\n","optimizer1 = GradientDescent(learning_rate=.001)\n","model1 = MLPReluDropout(M=256, num_classes=10)\n","y_pred, batch_acc = model1.fit(x_train[:10000], y_train[:10000], optimizer1)\n","y_test_pred = model1.predict(x_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"iK8fSGVhgp6O","outputId":"b0dd95a6-8b5b-484f-91f9-a2903c825164"},"outputs":[{"name":"stderr","output_type":"stream","text":["No handles with labels found to put in legend.\n"]},{"name":"stdout","output_type":"stream","text":["Number of full training batch iterations: 800\n","Accuracies per training batch:\n","4.6875\n","10.613019169329073\n","10.283546325878595\n","10.323482428115016\n","10.253594249201278\n","10.223642172523961\n","11.222044728434504\n","10.353434504792332\n","10.53314696485623\n","10.702875399361023\n","10.782747603833865\n","10.023961661341852\n","10.742811501597444\n","10.952476038338657\n","10.632987220447284\n","10.243610223642172\n","11.25199680511182\n","10.872603833865815\n","10.872603833865815\n","10.513178913738018\n","10.88258785942492\n","10.40335463258786\n","11.022364217252397\n","10.75279552715655\n","10.463258785942491\n","10.413338658146964\n","10.762779552715655\n","10.593051118210862\n","11.062300319488818\n","11.182108626198083\n","10.483226837060704\n","10.892571884984026\n","10.473242811501597\n","11.242012779552716\n","10.99241214057508\n","11.172124600638977\n","10.583067092651758\n","11.521565495207668\n","11.271964856230031\n","10.413338658146964\n","11.132188498402556\n","11.10223642172524\n","10.822683706070288\n","11.311900958466454\n","10.333466453674122\n","10.982428115015974\n","11.032348242811501\n","11.501597444089457\n","10.932507987220447\n","11.052316293929712\n","11.920926517571885\n","11.601437699680512\n","11.321884984025559\n","11.69129392971246\n","11.741214057507987\n","11.261980830670927\n","11.811102236421725\n","11.721246006389777\n","10.88258785942492\n","11.581469648562301\n","11.741214057507987\n","10.99241214057508\n","12.320287539936102\n","12.030750798722044\n","11.91094249201278\n","11.541533546325878\n","11.611421725239616\n","11.681309904153355\n","11.541533546325878\n","11.571485623003195\n","12.579872204472844\n","12.300319488817891\n","11.851038338658148\n","12.559904153354633\n","11.421725239616613\n","12.599840255591054\n","11.990814696485623\n","12.460063897763579\n","12.300319488817891\n","12.270367412140574\n","12.26038338658147\n","12.440095846645367\n","11.751198083067093\n","12.420127795527156\n","12.160543130990416\n","12.619808306709265\n","12.250399361022364\n","12.220447284345047\n","13.138977635782748\n","12.250399361022364\n","12.24041533546326\n","12.699680511182109\n","13.19888178913738\n","11.990814696485623\n","12.470047923322683\n","13.079073482428115\n","13.348642172523961\n","12.569888178913738\n","13.428514376996805\n","12.220447284345047\n","13.188897763578275\n","12.569888178913738\n","12.589856230031948\n","12.160543130990416\n","12.490015974440894\n","12.350239616613418\n","13.039137380191693\n","12.809504792332268\n","12.689696485623003\n","12.999201277955272\n","12.879392971246006\n","13.28873801916933\n","12.799520766773163\n","12.589856230031948\n","13.378594249201278\n","13.348642172523961\n","12.909345047923322\n","12.769568690095847\n","13.109025559105431\n","13.468450479233226\n","13.458466453674122\n","13.218849840255592\n","13.228833865814696\n","13.678115015974441\n","12.899361022364218\n","13.458466453674122\n","13.298722044728434\n","12.819488817891374\n","13.358626198083067\n","13.029153354632587\n","13.0491214057508\n","13.238817891373802\n","13.28873801916933\n","14.127396166134185\n","13.28873801916933\n","13.638178913738018\n","13.109025559105431\n","13.278753993610223\n","14.177316293929712\n","12.909345047923322\n","13.728035143769969\n","13.817891373801917\n","14.31709265175719\n","13.488418530351439\n","13.80790734824281\n","14.367012779552716\n","14.396964856230031\n","13.757987220447284\n","13.668130990415335\n","13.98761980830671\n","14.13738019169329\n","14.416932907348242\n","14.02755591054313\n","14.616613418530351\n","13.498402555910543\n","13.688099041533546\n","14.57667731629393\n","13.76797124600639\n","14.586661341853036\n","13.668130990415335\n","14.31709265175719\n","14.676517571884984\n","14.55670926517572\n","14.896166134185304\n","14.167332268370608\n","14.31709265175719\n","13.857827476038338\n","14.79632587859425\n","13.718051118210862\n","14.396964856230031\n","14.636581469648561\n","14.656549520766774\n","14.22723642172524\n","14.257188498402556\n","14.966054313099042\n","14.826277955271566\n","14.566693290734824\n","14.696485623003195\n","15.025958466453675\n","15.435303514376997\n","14.706469648562301\n","14.986022364217252\n","15.345447284345047\n","14.546725239616613\n","14.237220447284345\n","15.345447284345047\n","14.416932907348242\n","15.245607028753994\n","14.59664536741214\n","14.846246006389777\n","14.496805111821086\n","15.315495207667732\n","15.16573482428115\n","15.495207667731629\n","15.36541533546326\n","15.495207667731629\n","15.045926517571885\n","14.546725239616613\n","15.565095846645367\n","15.455271565495208\n","15.27555910543131\n","14.516773162939296\n","15.105830670926517\n","15.634984025559106\n","15.714856230031948\n","15.495207667731629\n","15.694888178913738\n","15.874600638977636\n","15.664936102236421\n","15.984424920127795\n","15.29552715654952\n","15.415335463258787\n","15.305511182108626\n","16.10423322683706\n","15.135782747603834\n","15.914536741214057\n","16.01437699680511\n","15.525159744408946\n","15.395367412140574\n","15.285543130990416\n","15.73482428115016\n","15.375399361022364\n","15.245607028753994\n","15.994408945686901\n","16.463658146964857\n","16.124201277955272\n","16.673322683706072\n","15.684904153354633\n","15.88458466453674\n","16.683306709265175\n","16.054313099041533\n","16.942891373801917\n","16.533546325878593\n","15.894568690095847\n","16.30391373801917\n","16.873003194888177\n","16.114217252396166\n","16.34384984025559\n","16.573482428115017\n","16.5435303514377\n","16.7232428115016\n","16.353833865814696\n","17.072683706070286\n","16.982827476038338\n","16.603434504792332\n","16.882987220447284\n","17.312300319488816\n","16.573482428115017\n","16.773162939297123\n","17.20247603833866\n","16.663338658146966\n","16.65335463258786\n","16.553514376996805\n","17.422124600638977\n","16.5435303514377\n","16.623402555910545\n","18.190894568690094\n","16.793130990415335\n","17.292332268370608\n","16.882987220447284\n","18.041134185303516\n","17.072683706070286\n","17.192492012779553\n","17.142571884984026\n","17.032747603833865\n","17.052715654952078\n","17.52196485623003\n","17.35223642172524\n","17.68170926517572\n","17.402156549520768\n","17.61182108626198\n","17.362220447284344\n","18.111022364217252\n","17.94129392971246\n","17.0926517571885\n","17.631789137380192\n","17.96126198083067\n","17.00279552715655\n","17.142571884984026\n","17.142571884984026\n","17.85143769968051\n","17.661741214057507\n","18.650159744408946\n","18.041134185303516\n","18.111022364217252\n","17.781549520766774\n","17.771565495207668\n","18.061102236421725\n","18.05111821086262\n","18.101038338658146\n","18.220846645367413\n","18.190894568690094\n","17.701677316293928\n","18.580271565495206\n","17.55191693290735\n","17.771565495207668\n","18.041134185303516\n","18.570287539936103\n","18.55031948881789\n","18.480431309904155\n","17.432108626198083\n","17.981230031948883\n","18.460463258785943\n","19.039536741214057\n","18.630191693290733\n","19.26916932907348\n","18.53035143769968\n","18.450479233226837\n","19.418929712460063\n","19.159345047923324\n","17.92132587859425\n","19.279153354632587\n","18.929712460063897\n","17.891373801916934\n","18.779952076677315\n","17.981230031948883\n","18.939696485623003\n","18.999600638977636\n","19.069488817891372\n","18.560303514376997\n","19.039536741214057\n","20.01797124600639\n","19.039536741214057\n","19.50878594249201\n","19.09944089456869\n","19.438897763578275\n","20.16773162939297\n","19.61861022364217\n","18.869808306709267\n","19.49880191693291\n","19.44888178913738\n","19.069488817891372\n","19.638578274760384\n","18.94968051118211\n","19.768370607028753\n","19.49880191693291\n","19.598642172523963\n","19.918130990415335\n","20.117811501597444\n","19.748402555910545\n","19.728434504792332\n","19.53873801916933\n","20.207667731629392\n","20.307507987220447\n","18.87979233226837\n","20.487220447284344\n","20.517172523961662\n","19.628594249201278\n","20.587060702875398\n","20.097843450479232\n","19.878194888178914\n","20.307507987220447\n","20.287539936102238\n","20.56709265175719\n","19.22923322683706\n","20.656948881789138\n","20.906549520766774\n","20.906549520766774\n","20.617012779552716\n","20.71685303514377\n","20.527156549520768\n","19.938099041533548\n","20.986421725239616\n","20.686900958466452\n","20.197683706070286\n","20.317492012779553\n","20.507188498402556\n","20.626996805111823\n","20.137779552715656\n","19.88817891373802\n","20.7767571884984\n","21.106230031948883\n","20.2176517571885\n","21.036341853035143\n","20.437300319488816\n","21.30591054313099\n","21.685303514376997\n","21.00638977635783\n","21.11621405750799\n","20.347444089456868\n","20.626996805111823\n","20.826677316293928\n","21.405750798722046\n","20.067891373801917\n","21.5055910543131\n","21.35583067092652\n","20.247603833865814\n","20.25758785942492\n","21.994808306709267\n","21.98482428115016\n","21.106230031948883\n","22.104632587859424\n","22.124600638977636\n","21.894968051118212\n","22.124600638977636\n","21.3258785942492\n","21.19608626198083\n","21.455670926517573\n","21.465654952076676\n","20.626996805111823\n","20.88658146964856\n","21.39576677316294\n","20.80670926517572\n","22.09464856230032\n","21.335862619808307\n","22.174520766773163\n","21.96485623003195\n","21.455670926517573\n","21.48562300319489\n","21.705271565495206\n","22.11461661341853\n","22.18450479233227\n","22.893370607028753\n","22.234424920127797\n","21.475638977635782\n","21.91493610223642\n","21.685303514376997\n","22.7935303514377\n","22.454073482428115\n","21.924920127795527\n","22.643769968051117\n","22.064696485623003\n","22.81349840255591\n","22.94329073482428\n","21.865015974440894\n","21.735223642172524\n","22.81349840255591\n","22.57388178913738\n","23.60223642172524\n","21.605431309904155\n","23.023162939297123\n","22.693690095846645\n","23.043130990415335\n","23.45247603833866\n","23.552316293929714\n","22.713658146964857\n","23.003194888178914\n","23.162939297124602\n","22.404153354632587\n","22.603833865814696\n","22.164536741214057\n","22.843450479233226\n","22.753594249201278\n","22.81349840255591\n","23.222843450479232\n","23.192891373801917\n","22.284345047923324\n","23.642172523961662\n","22.18450479233227\n","23.542332268370608\n","23.232827476038338\n","23.58226837060703\n","24.19129392971246\n","23.07308306709265\n","24.121405750798722\n","23.881789137380192\n","24.660543130990416\n","23.302715654952078\n","23.97164536741214\n","23.93170926517572\n","23.921725239616613\n","23.25279552715655\n","23.981629392971247\n","23.612220447284344\n","23.60223642172524\n","24.21126198083067\n","23.702076677316295\n","24.21126198083067\n","23.372603833865814\n","22.94329073482428\n","24.43091054313099\n","24.10143769968051\n","24.580670926517573\n","23.751996805111823\n","23.043130990415335\n","23.192891373801917\n","24.550718849840255\n","24.30111821086262\n","23.742012779552716\n","24.361022364217252\n","24.04153354632588\n","23.412539936102238\n","23.712060702875398\n","25.289536741214057\n","24.6305910543131\n","24.790335463258785\n","24.031549520766774\n","23.761980830670925\n","25.38937699680511\n","24.840255591054312\n","24.76038338658147\n","24.590654952076676\n","24.311102236421725\n","25.119808306709267\n","25.908546325878593\n","24.21126198083067\n","24.980031948881788\n","24.201277955271564\n","25.159744408945688\n","25.139776357827476\n","25.19968051118211\n","24.740415335463258\n","24.6305910543131\n","25.249600638977636\n","25.27955271565495\n","25.30950479233227\n","25.249600638977636\n","25.928514376996805\n","26.297923322683705\n","25.299520766773163\n","24.580670926517573\n","24.950079872204473\n","25.499201277955272\n","25.648961661341854\n","25.299520766773163\n","25.099840255591054\n","24.91014376996805\n","26.108226837060702\n","26.06829073482428\n","25.359424920127797\n","25.908546325878593\n","25.638977635782748\n","26.367811501597444\n","26.347843450479232\n","26.30790734824281\n","24.840255591054312\n","25.69888178913738\n","25.56908945686901\n","25.299520766773163\n","25.708865814696484\n","25.728833865814696\n","25.58905750798722\n","25.489217252396166\n","25.469249201277954\n","26.058306709265175\n","26.677316293929714\n","25.27955271565495\n","25.419329073482427\n","25.0\n","27.356230031948883\n","26.737220447284344\n","25.858626198083066\n","26.457667731629392\n","26.587460063897762\n","26.827076677316295\n","26.0982428115016\n","26.567492012779553\n","26.886980830670925\n","26.996805111821086\n","26.347843450479232\n","26.587460063897762\n","27.29632587859425\n","26.437699680511184\n","25.768769968051117\n","26.697284345047922\n","26.627396166134186\n","27.55591054313099\n","27.196485623003195\n","26.587460063897762\n","26.22803514376997\n","27.05670926517572\n","27.31629392971246\n","27.805511182108628\n","27.515974440894567\n","27.36621405750799\n","26.886980830670925\n","27.525958466453673\n","26.757188498402556\n","27.31629392971246\n","27.90535143769968\n","26.906948881789138\n","26.597444089456868\n","27.655750798722046\n","27.635782747603834\n","27.68570287539936\n","27.765575079872203\n","28.444488817891372\n","27.386182108626198\n","27.945287539936103\n","26.447683706070286\n","26.847044728434504\n","28.47444089456869\n","27.84544728434505\n","27.29632587859425\n","26.39776357827476\n","28.25479233226837\n","27.266373801916934\n","28.773961661341854\n","28.065095846645367\n","27.515974440894567\n","28.125\n","27.156549520766774\n","28.184904153354633\n","28.294728434504794\n","28.424520766773163\n","28.115015974440894\n","27.13658146964856\n","27.595846645367413\n","27.356230031948883\n","28.19488817891374\n","28.51437699680511\n","27.16653354632588\n","28.304712460063897\n","27.016773162939298\n","28.115015974440894\n","29.163338658146966\n","27.61581469648562\n","27.73562300319489\n","27.286341853035143\n","28.095047923322685\n","28.624201277955272\n","28.414536741214057\n","27.765575079872203\n","28.47444089456869\n","28.574281150159745\n","28.105031948881788\n","27.77555910543131\n","28.534345047923324\n","28.32468051118211\n","28.863817891373802\n","28.71405750798722\n","28.51437699680511\n","28.32468051118211\n","29.512779552715656\n","27.79552715654952\n","29.203274760383387\n","29.54273162939297\n","28.724041533546327\n","29.492811501597444\n","28.773961661341854\n","28.34464856230032\n","28.99361022364217\n","28.813897763578275\n","28.554313099041533\n","28.863817891373802\n","28.374600638977636\n","29.52276357827476\n","29.26317891373802\n","29.452875399361023\n","29.902156549520768\n","28.813897763578275\n","29.512779552715656\n","30.041932907348244\n","29.452875399361023\n","29.46285942492013\n","29.922124600638977\n","29.472843450479232\n","29.15335463258786\n","29.203274760383387\n","29.91214057507987\n","29.382987220447284\n","28.893769968051117\n","29.932108626198083\n","29.562699680511184\n","30.68091054313099\n","29.662539936102238\n","30.05191693290735\n","30.11182108626198\n","30.670926517571885\n","29.422923322683705\n","30.62100638977636\n","30.121805111821086\n","30.79073482428115\n","29.862220447284344\n","30.57108626198083\n","29.882188498402556\n","30.1517571884984\n","30.451277955271564\n","30.601038338658146\n","29.812300319488816\n","29.842252396166135\n","30.18170926517572\n","30.3314696485623\n","29.5926517571885\n","30.42132587859425\n","30.66094249201278\n","30.271565495207668\n","30.42132587859425\n","31.01038338658147\n","29.892172523961662\n","29.073482428115017\n","30.201677316293928\n","30.391373801916934\n","30.121805111821086\n","30.53115015974441\n","31.21006389776358\n","31.000399361022364\n","30.22164536741214\n","31.130191693290733\n","30.640974440894567\n","30.341453674121407\n","30.53115015974441\n","30.68091054313099\n","30.471246006389777\n","29.333067092651756\n","30.481230031948883\n","30.101837060702877\n","31.48961661341853\n","30.131789137380192\n","30.57108626198083\n","31.130191693290733\n","31.000399361022364\n","31.87899361022364\n","31.869009584664536\n","31.719249201277954\n","30.18170926517572\n","30.7008785942492\n","31.33985623003195\n","31.888977635782748\n","31.519568690095845\n","31.499600638977636\n","30.780750798722046\n","31.14017571884984\n","31.16014376996805\n","31.269968051118212\n","31.070287539936103\n","31.16014376996805\n","31.190095846645367\n","30.521166134185304\n","31.70926517571885\n","31.75918530351438\n","32.08865814696485\n","32.308306709265175\n","30.341453674121407\n","31.83905750798722\n","32.1785143769968\n","31.389776357827476\n","31.988817891373802\n","32.22843450479233\n","31.7991214057508\n","31.150159744408946\n","32.418130990415335\n","31.988817891373802\n","32.198482428115014\n","31.39976038338658\n","32.00878594249201\n","31.499600638977636\n","32.677715654952074\n","31.589456869009584\n","32.1785143769968\n","32.42811501597444\n","31.659345047923324\n","31.988817891373802\n","31.419728434504794\n","32.14856230031949\n","32.08865814696485\n","32.897364217252395\n","31.539536741214057\n","32.048722044728436\n","32.60782747603834\n","32.45806709265176\n","32.09864217252396\n","32.74760383386582\n","32.38817891373802\n","32.08865814696485\n","32.37819488817891\n","32.65774760383387\n","32.76757188498402\n","32.727635782747605\n","32.82747603833866\n","32.99720447284345\n","33.426517571884986\n","32.9073482428115\n","32.70766773162939\n","32.74760383386582\n","33.037140575079874\n","33.546325878594246\n","32.45806709265176\n","32.308306709265175\n","33.02715654952077\n","33.546325878594246\n","33.206869009584665\n","33.55630990415335\n","32.9073482428115\n","33.4464856230032\n","32.837460063897765\n","33.19688498402556\n","33.486421725239616\n","32.727635782747605\n","33.22683706070288\n","32.70766773162939\n","32.97723642172524\n","34.28514376996805\n","33.905750798722046\n","32.50798722044728\n","33.55630990415335\n","32.947284345047926\n","32.448083067092654\n","33.736022364217256\n","33.55630990415335\n","32.75758785942492\n","32.897364217252395\n","32.79752396166134\n","33.156948881789134\n","33.885782747603834\n","33.58626198083067\n","33.576277955271564\n","33.8158945686901\n","33.32667731629393\n","33.2767571884984\n","---------\n"]},{"data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["57.02\n"]}],"source":["print(\"Number of full training batch iterations:\",len(batch_acc))\n","print(\"Accuracies per training batch:\")\n","for i in batch_acc:\n","  print(i)\n","print(\"---------\")\n","fig, ax = plt.subplots()\n","ax.scatter(range(len(batch_acc)), batch_acc, c='r', marker='x')\n","ax.legend()\n","ax.grid(True)\n","plt.ylabel('accuracy')\n","plt.xlabel('epoch')\n","plt.show()\n","print(evaluate_acc(y_test_pred, y_test))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"VbyAzGIMnKq6","outputId":"01a1120c-5c05-4c7b-b262-73eff0277499"},"outputs":[{"data":{"text/plain":["57.02"]},"execution_count":null,"metadata":{},"output_type":"execute_result"}],"source":["evaluate_acc(y_test_pred, y_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":298,"status":"ok","timestamp":1648761884566,"user":{"displayName":"Clem G","userId":"09532762742774460995"},"user_tz":240},"id":"YohPYX-phVfo","outputId":"170d8022-b67d-434a-b4bb-eb41410e9cb8"},"outputs":[{"data":{"text/plain":["62.12"]},"execution_count":94,"metadata":{},"output_type":"execute_result"}],"source":["accM1L1 = evaluate_acc(y_test_pred, y_test)\n","accM1L1"]},{"cell_type":"markdown","metadata":{"id":"ha-iowhL0aMk"},"source":["## Model 2 - 1 hidden layer\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CbDYOzBf3edj"},"outputs":[],"source":["model2 = MLP2Layer(M=128, num_classes=10)\n","y_pred = model2.fit(x_train[:100], y_train[:100], optimizer1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RVCMlNm33eT1"},"outputs":[],"source":["y_test_pred = model1.predict(x_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":157,"status":"ok","timestamp":1648762989643,"user":{"displayName":"Seraphin Bassas","userId":"17841861031575039122"},"user_tz":240},"id":"F3JvOO-z3eEJ","outputId":"013a8a6f-a4cd-44f9-f0b2-b906fa299373"},"outputs":[{"data":{"text/plain":["55.25"]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["accM1L1 = evaluate_acc(y_test_pred, y_test)\n","accM1L1"]},{"cell_type":"markdown","metadata":{"id":"_4PlmUW07tZJ"},"source":["**k-fold cross validation**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"M-AvsoUlURrA","outputId":"a77842c7-5c46-41cc-8b4d-2a679911690a"},"outputs":[{"name":"stdout","output_type":"stream","text":["66.0 accuracy for M=128, nonlinearity=<function relu at 0x7f901e921ef0>, lr=0.001, batch size=16.\n","68.0 accuracy for M=128, nonlinearity=<function relu at 0x7f901e921ef0>, lr=0.002, batch size=16.\n","64.0 accuracy for M=128, nonlinearity=<function relu at 0x7f901e921ef0>, lr=0.004, batch size=16.\n","69.0 accuracy for M=128, nonlinearity=<function relu at 0x7f901e921ef0>, lr=0.001, batch size=32.\n","66.0 accuracy for M=128, nonlinearity=<function relu at 0x7f901e921ef0>, lr=0.002, batch size=32.\n","67.0 accuracy for M=128, nonlinearity=<function relu at 0x7f901e921ef0>, lr=0.004, batch size=32.\n"]}],"source":["hyper_tuning(MLP3Layer, activ = relu, x_train=x_train[:100], y_train=y_train[:100])"]},{"cell_type":"markdown","metadata":{"id":"8MF-zQy93e_3"},"source":["# Model 3 - 2 Hidden Layers\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mIM45Mk73kKH"},"outputs":[],"source":["tuningAccRelu = hyper_tuning(MLPLeakyRelu, activ = relu, x_train=x_train[:100], y_train=y_train[:100])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dolEq54r3kk6"},"outputs":[],"source":["hyper_tuning(MLPLeakyRelu, activ = relu, x_train=x_train[:100], y_train=y_train[:100])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pfaZ61443kYF"},"outputs":[],"source":[""]}],"metadata":{"colab":{"collapsed_sections":[],"name":"CombinedMLPs.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.6"}},"nbformat":4,"nbformat_minor":0}