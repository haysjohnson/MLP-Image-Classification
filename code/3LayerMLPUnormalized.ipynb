{"cells":[{"cell_type":"markdown","metadata":{"id":"aaAfLFKjLG2w"},"source":["[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/mravanba/comp551-notebooks/blob/master/MLP.ipynb)\n","\n","# Multilayer Perceptron (MLP)\n","Our goal here is to implement a two-layer neural network for binary classification, train it using gradient descent and use it to classify the Iris dataset.\n","Our model is\n","$$\n","\\hat{y} = \\sigma \\left ( W \\sigma \\left ( V x \\right ) \\right)\n","$$\n","where we have $M$ hidden units and $D$ input features -- that is $w \\in \\mathbb{R}^{M}$, and $V \\in \\mathbb{R}^{M \\times D}$. For simplicity here we do not include a bias parameter for each layer. Key to our implementation is the gradient calculation. We follow the notation used in the slides here."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"r1SqxPqxLG2_"},"outputs":[],"source":["import numpy as np\n","#%matplotlib notebook\n","%matplotlib inline\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","\n","from IPython.core.debugger import set_trace\n","from tensorflow import keras\n","import warnings\n","warnings.filterwarnings('ignore')\n","from sklearn.model_selection import KFold"]},{"cell_type":"markdown","metadata":{"id":"nPNwId5c2VO_"},"source":["# Data Preprocessing"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2536,"status":"ok","timestamp":1649098179391,"user":{"displayName":"Seraphin Bassas","userId":"17841861031575039122"},"user_tz":240},"id":"phWN5w-7GJOK","outputId":"2699e5e5-3eb5-473f-cddf-c4481aed6591"},"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n","32768/29515 [=================================] - 0s 0us/step\n","40960/29515 [=========================================] - 0s 0us/step\n","Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n","26427392/26421880 [==============================] - 1s 0us/step\n","26435584/26421880 [==============================] - 1s 0us/step\n","Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n","16384/5148 [===============================================================================================] - 0s 0us/step\n","Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n","4423680/4422102 [==============================] - 0s 0us/step\n","4431872/4422102 [==============================] - 0s 0us/step\n"]}],"source":["# packaging it all into a function\n","def preprocess_fashion_mnist():\n","  import random as rand\n","   \n","   \n","  (x_train, y_train), (x_test, y_test) = keras.datasets.fashion_mnist.load_data()\n","  mean_mat = np.mean(x_train, axis=0)\n","  \n","  # centering the data by removing the pixel wise mean from every pixel in every image\n","  x_train_centered = x_train - mean_mat\n","  x_test_centered = x_test - mean_mat\n","  \n","  #finally, flattening the data\n","  x_train = np.reshape(x_train_centered, (60000,784))\n","  x_test = np.reshape(x_test_centered, (10000, 784))\n","  #converting the test data to one hot encodings\n","  y_train = keras.utils.to_categorical(y_train, num_classes=10)\n","  y_test = keras.utils.to_categorical(y_test, num_classes=10)\n","  \n","  return x_train[:10000], y_train[:10000], x_test, y_test\n","x_train, y_train, x_test, y_test = preprocess_fashion_mnist()"]},{"cell_type":"markdown","metadata":{"id":"viQL0Q9L2dAM"},"source":["# Model Implementation - SoftMax\n"]},{"cell_type":"markdown","metadata":{"id":"oGwQgzDZ6XaO"},"source":["**Activation functions**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dxQ35WhEQLdR"},"outputs":[],"source":["#activation functions\n","softmax1D = lambda z: np.exp(z)/float(sum(np.exp(z)))\n","softmax2D = lambda z: np.array([np.exp(i)/float(sum(np.exp(i))) for i in z])\n","\n","# Logistic\n","logistic = lambda z: 1./ (1 + np.exp(-z))\n","\n","# Tanh\n","tanh = lambda x: 2./ (1+np.exp(-2*x)) -1\n","tanh_grad = lambda x: 1 - np.square(2./ (1+np.exp(-2*x)) -1)\n","\n","# Leaky ReLu\n","def leaky_relu(x):\n","  alpha = 0.1\n","  x=np.array(x).astype(float)\n","  np.putmask(x, x<0, alpha*x)\n","  return x\n","\n","def leaky_relu_grad(x):\n","  alpha = 0.1\n","  x=np.array(x).astype(float)\n","  x[x>0]=1\n","  x[x<=0]=alpha\n","  return x\n","\n","  \n","# ReLu\n","def relu(x):\n","  x=np.array(x).astype(float)\n","  np.putmask(x, x<0, 0)\n","  return x\n","  \n","def relu_grad(x):\n","  x=np.array(x).astype(float)\n","  x[x>0]=1\n","  x[x<=0]=0\n","  return x"]},{"cell_type":"markdown","metadata":{"id":"h524tYb46mwq"},"source":["Accuracy function"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hgeOx3cLuwEl"},"outputs":[],"source":["def evaluate_acc(pred, truth):\n","  counter =0\n","  \n","  for i in range(len(pred)):\n","    maxVal = np.where(pred[i] == np.amax(pred[i]))\n","    counter += 1 if maxVal == np.where(truth[i]==1) else 0\n","  return counter * 100.0 / float(len(pred))\n","  "]},{"cell_type":"markdown","metadata":{"id":"2sFWzkOs6pJ4"},"source":["## Model - ReLu"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"a9CPhDEuLG3C"},"outputs":[],"source":["class MLPRelu:\n","    \n","    def __init__(self, M = 128, num_classes = 10):\n","        self.M = M\n","        self.num_classes = num_classes\n","        \n","            \n","    def fit(self, x, y, optimizer):\n","        N,D = x.shape\n","        def gradient(x, y, params):\n","            \n","            v1, v2, w = params # v1.shape = (D, M), v2.shape = (M, M) w.shape = (M)\n","            q1 = np.dot(x, v1) \n","            z1 = relu(q1) #N x M\n","            q2 = np.dot(z1, v2) # N\n","            z2 = relu(q2)          \n","            yh = softmax2D(np.dot(z2, w))#N\n","            train_acc = evaluate_acc(yh, y)\n","            ## Backpropagation          \n","            dy = yh - y #N\n","            dw = np.dot(z2.T, dy)/N #M  \n","\n","            ## 2nd Layer                  \n","            # dz = np.dot(dy.T, w)\n","            dz2 = np.dot(dy, w.numpy().T) #N x M                   = (yh-y)*w from slide 16\n","            dv2 = np.dot(z1.T, dz2 * relu_grad(q2))/N #D x M   = (yh-y)*w*(activation)'*x\n","            \n","            ## 3rd Layer\n","            dz1 = np.dot(dz2, v2.numpy().T) #N x M                   = (yh-y)*w from slide 16\n","            dv1 = np.dot(x.T, dz1 * relu_grad(q1))/N #D x M   = (yh-y)*w*(activation)'*x\n","\n","            dparams = [dv1, dv2, dw]\n","            return dparams, train_acc\n","        \n","        # w = np.random.randn(self.M) * .01\n","        # v = np.random.randn(D,self.M) * .01\n","        initializer = keras.initializers.GlorotNormal()\n","        w = initializer(shape=(self.M, self.num_classes))\n","        v2 = initializer(shape=(self.M, self.M))\n","        v1 = initializer(shape=(D, self.M))\n","        \n","        params0 = [v1, v2,w]\n","        self.params, batch_train_accs = optimizer.run_mini_batch(gradient, x, y, params0) \n","        return self, batch_train_accs\n","    \n","    def predict(self, x):\n","        v1, v2, w = self.params\n","        z1 = relu(np.dot(x, v1)) #N x M\n","        z2 = relu(np.dot(z1, v2))\n","        yh = softmax2D(np.dot(z2, w))#N\n","        return yh"]},{"cell_type":"markdown","metadata":{"id":"o07x1kJe21uP"},"source":["# Batch Implementation"]},{"cell_type":"markdown","metadata":{"id":"WON0hiNiLG3E"},"source":["In the implementation above we have used a list data structure to maintain model parameters and their gradients. Below I have modified the `GradientDescent` class to also work with a list of parameters. One sournce of confusion in the above implementation is the gradient calculation. While in the slides during the lectures \n","we calculated the partial derivative for individual parameters, here, we use vector and matrix operations to calculate the derivative for *all* parameters. "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"edVLJC0tqSpA"},"outputs":[],"source":["def mini_batcher(x, y, mini_batch_size):\n","  zipped = np.hstack( (x, y ) )\n","  np.random.shuffle(zipped)\n","  x_batches, y_batches = [], []\n","  mini_batches = []\n","  batch_num = x.shape[0] // mini_batch_size \n","  for i in range(batch_num):\n","    x_batch = zipped[ i * mini_batch_size : (i+1) * mini_batch_size, :-10]\n","    y_batch = zipped[ i * mini_batch_size : (i+1) * mini_batch_size, -10:]\n","    mini_batches.append( ( x_batch, y_batch) )\n","  if x.shape[0] % mini_batch_size != 0:\n","    x_batch = zipped[ batch_num * mini_batch_size :, :-10]\n","    y_batch = zipped[ batch_num * mini_batch_size :, -10:]\n","    mini_batches.append( ( x_batch, y_batch ) )\n","  return mini_batches"]},{"cell_type":"markdown","metadata":{"id":"rt5K2LpN3KcI"},"source":["# Gradient Descent \n","\n","---\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LRgwO0YHLG3E"},"outputs":[],"source":["class GradientDescent:\n","    \n","    def __init__(self, learning_rate=.001, max_iters=800, epsilon=1e-8, batch_size=32):\n","        self.learning_rate = learning_rate\n","        self.max_iters = max_iters\n","        self.epsilon = epsilon\n","        \n","    def run(self, gradient_fn, x, y, params):\n","        norms = np.array([np.inf])\n","        t = 1\n","        while np.any(norms > self.epsilon) and t < self.max_iters:\n","            grad = gradient_fn(x, y, params)\n","            for p in range(len(params)):\n","                params[p] -= self.learning_rate * grad[p]\n","            t += 1\n","            norms = np.array([np.linalg.norm(g) for g in grad])\n","        print(t)\n","        return params\n","    \n","    def run_mini_batch(self, gradient_fn, x, y, params, batch_size=32):\n","        batch_train_acc, chunk = [], []\n","        norms = np.array([np.inf])\n","        t=1\n","        mini_batches = mini_batcher(x, y, batch_size)\n","        print(\"#mini batches\",len(mini_batches))\n","        batch_index=0\n","        while np.any(norms > self.epsilon) and t < self.max_iters * len(mini_batches):\n","            x_temp, y_temp = mini_batches[t % ( len(mini_batches)-1 ) ][0], mini_batches[t % ( len(mini_batches)-1 ) ][1]\n","            \n","            grad, temp_acc = gradient_fn(x_temp, y_temp, params)\n","            if(batch_index == batch_size):\n","              mini_batches = mini_batcher(x, y, batch_size)\n","              batch_index = 0\n","            batch_index +=1\n","            for p in range(len(params)):\n","                params[p] -= self.learning_rate * grad[p]\n","            chunk.append(temp_acc)\n","            if t % 10000 == 0:\n","              print(f\"Epoch{t}:{temp_acc}%\")\n","            t += 1\n","            if t%len(mini_batches) == 0:\n","              batch_train_acc.append(np.mean(chunk))\n","              chunk = []\n","            norms = np.array([np.linalg.norm(g) for g in grad])\n","        return params, batch_train_acc"]},{"cell_type":"markdown","metadata":{"id":"Sx13m2QLRjZF"},"source":["# MNIST DataSet"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WCMi5fHmFEfY","executionInfo":{"status":"ok","timestamp":1649103215805,"user_tz":240,"elapsed":1703832,"user":{"displayName":"Seraphin Bassas","userId":"17841861031575039122"}},"outputId":"508eceb7-ddaa-41ed-8fba-2f1fb2d5719e"},"outputs":[{"output_type":"stream","name":"stdout","text":["#mini batches 313\n","Epoch10000:65.625%\n","Epoch20000:93.75%\n","Epoch30000:87.5%\n","Epoch40000:87.5%\n","Epoch50000:81.25%\n","Epoch60000:87.5%\n","Epoch70000:90.625%\n","Epoch80000:93.75%\n","Epoch90000:93.75%\n","Epoch100000:96.875%\n","Epoch110000:96.875%\n","Epoch120000:96.875%\n","Epoch130000:96.875%\n","Epoch140000:96.875%\n","Epoch150000:100.0%\n","Epoch160000:100.0%\n","Epoch170000:100.0%\n","Epoch180000:96.875%\n","Epoch190000:100.0%\n","Epoch200000:100.0%\n","Epoch210000:100.0%\n","Epoch220000:100.0%\n","Epoch230000:100.0%\n","Epoch240000:100.0%\n","Epoch250000:100.0%\n"]}],"source":["model1 = MLPRelu(M=128, num_classes=10)\n","optimizer = GradientDescent(learning_rate=.002, batch_size=32)\n","y_pred1, batch_train_accs = model1.fit(x_train, y_train, optimizer)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"h_yOflLSFKXU","executionInfo":{"status":"ok","timestamp":1649099470332,"user_tz":240,"elapsed":692,"user":{"displayName":"Seraphin Bassas","userId":"17841861031575039122"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"36c34f1d-9585-4455-946a-9af7f3722187"},"outputs":[{"output_type":"stream","name":"stdout","text":["Number of full training batch iterations: 800\n","Accuracies per training batch:\n","32.39182692307692\n","55.91054313099041\n","62.41014376996805\n","65.0758785942492\n","66.97284345047923\n","68.370607028754\n","69.75838658146965\n","70.65694888178913\n","71.48562300319489\n","72.00479233226837\n","72.75359424920129\n","73.1729233226837\n","73.72204472843451\n","74.33107028753993\n","74.78035143769968\n","75.25958466453675\n","75.66892971246007\n","75.88857827476038\n","76.24800319488818\n","76.50758785942492\n","76.6673322683706\n","77.0167731629393\n","77.2064696485623\n","77.50599041533546\n","77.67571884984025\n","77.77555910543131\n","78.05511182108626\n","78.19488817891374\n","78.52436102236422\n","78.77396166134186\n","79.0435303514377\n","79.20327476038338\n","79.40295527156549\n","79.50279552715655\n","79.69249201277955\n","79.7623801916933\n","79.90215654952077\n","79.9820287539936\n","80.05191693290735\n","80.10183706070288\n","80.18170926517571\n","80.35143769968052\n","80.55111821086263\n","80.71086261980831\n","80.85063897763578\n","80.86062300319489\n","81.00039936102236\n","81.21006389776358\n","81.25\n","81.33985623003196\n","81.4297124600639\n","81.53953674121406\n","81.59944089456869\n","81.81908945686901\n","81.86900958466454\n","82.04872204472844\n","82.1785143769968\n","82.41813099041534\n","82.55790734824281\n","82.56789137380191\n","82.58785942492013\n","82.71765175718849\n","82.78753993610223\n","82.87739616613419\n","82.99720447284345\n","83.11701277955271\n","83.30670926517571\n","83.46645367412141\n","83.59624600638978\n","83.64616613418531\n","83.72603833865814\n","83.83586261980831\n","83.91573482428115\n","83.93570287539936\n","84.05551118210863\n","84.23522364217253\n","84.26517571884985\n","84.23522364217253\n","84.39496805111821\n","84.48482428115015\n","84.65455271565496\n","84.70447284345047\n","84.78434504792332\n","84.91413738019169\n","84.95407348242811\n","85.05391373801918\n","85.07388178913737\n","85.27356230031948\n","85.31349840255591\n","85.42332268370608\n","85.43330670926518\n","85.54313099041534\n","85.57308306709265\n","85.58306709265176\n","85.6729233226837\n","85.80271565495208\n","85.78274760383387\n","85.87260383386581\n","86.08226837060703\n","86.06230031948881\n","86.19209265175719\n","86.2020766773163\n","86.26198083067092\n","86.29193290734824\n","86.40175718849841\n","86.44169329073482\n","86.52156549520767\n","86.5814696485623\n","86.67132587859425\n","86.77116613418531\n","86.82108626198082\n","86.93091054313099\n","86.99081469648563\n","87.06070287539936\n","87.08067092651757\n","87.17052715654953\n","87.21046325878594\n","87.26038338658147\n","87.3202875399361\n","87.36022364217253\n","87.33027156549521\n","87.34025559105432\n","87.5099840255591\n","87.51996805111821\n","87.54992012779553\n","87.63977635782747\n","87.61980830670926\n","87.73961661341853\n","87.80950479233226\n","87.84944089456869\n","87.94928115015975\n","88.00918530351437\n","88.03913738019169\n","88.06908945686901\n","88.15894568690096\n","88.2388178913738\n","88.29872204472844\n","88.32867412140575\n","88.34864217252397\n","88.40854632587859\n","88.47843450479233\n","88.51837060702876\n","88.64816293929712\n","88.67811501597444\n","88.67811501597444\n","88.70806709265176\n","88.7679712460064\n","88.85782747603834\n","88.9476837060703\n","88.89776357827476\n","88.93769968051119\n","88.93769968051119\n","89.01757188498402\n","89.01757188498402\n","89.02755591054313\n","89.02755591054313\n","89.06749201277955\n","89.10742811501598\n","89.1673322683706\n","89.21725239616613\n","89.34704472843451\n","89.43690095846645\n","89.44688498402556\n","89.5067891373802\n","89.56669329073482\n","89.65654952076677\n","89.75638977635782\n","89.79632587859425\n","89.8761980830671\n","89.92611821086263\n","89.97603833865814\n","90.02595846645367\n","89.97603833865814\n","90.00599041533546\n","90.02595846645367\n","90.14576677316293\n","90.20567092651757\n","90.20567092651757\n","90.17571884984025\n","90.21565495207668\n","90.245607028754\n","90.27555910543131\n","90.29552715654953\n","90.34544728434504\n","90.37539936102236\n","90.46525559105432\n","90.58506389776358\n","90.57507987220447\n","90.60503194888179\n","90.65495207667732\n","90.66493610223642\n","90.75479233226837\n","90.71485623003196\n","90.77476038338658\n","90.8047124600639\n","90.84464856230032\n","90.89456869009585\n","90.93450479233226\n","90.97444089456869\n","91.03434504792332\n","91.09424920127796\n","91.14416932907348\n","91.20407348242811\n","91.24400958466454\n","91.28394568690096\n","91.31389776357827\n","91.34384984025559\n","91.44369009584665\n","91.43370607028754\n","91.45367412140575\n","91.48362619808307\n","91.5535143769968\n","91.5435303514377\n","91.64337060702876\n","91.66333865814697\n","91.70327476038338\n","91.7332268370607\n","91.76317891373802\n","91.85303514376997\n","91.86301916932908\n","91.8929712460064\n","91.90295527156549\n","92.00279552715655\n","92.00279552715655\n","92.0826677316294\n","92.1126198083067\n","92.17252396166134\n","92.20247603833866\n","92.24241214057508\n","92.31230031948881\n","92.33226837060703\n","92.39217252396166\n","92.43210862619809\n","92.52196485623003\n","92.57188498402556\n","92.65175718849841\n","92.68170926517571\n","92.73162939297124\n","92.68170926517571\n","92.76158146964856\n","92.86142172523962\n","92.84145367412141\n","92.89137380191693\n","92.94129392971246\n","92.92132587859425\n","92.93130990415335\n","93.08107028753993\n","93.10103833865814\n","93.06110223642173\n","93.14097444089457\n","93.21086261980831\n","93.29073482428115\n","93.30071884984025\n","93.36062300319489\n","93.34065495207668\n","93.3805910543131\n","93.43051118210863\n","93.46046325878594\n","93.47044728434504\n","93.55031948881789\n","93.5702875399361\n","93.58027156549521\n","93.62020766773163\n","93.69009584664536\n","93.72004792332268\n","93.7599840255591\n","93.77995207667732\n","93.79992012779553\n","93.81988817891374\n","93.88977635782747\n","93.91972843450479\n","93.9297124600639\n","93.98961661341853\n","93.99960063897764\n","94.05950479233226\n","94.05950479233226\n","94.08945686900958\n","94.09944089456869\n","94.1094249201278\n","94.129392971246\n","94.22923322683707\n","94.25918530351437\n","94.25918530351437\n","94.31908945686901\n","94.37899361022365\n","94.37899361022365\n","94.38897763578275\n","94.43889776357827\n","94.45886581469648\n","94.47883386581469\n","94.4988019169329\n","94.47883386581469\n","94.51876996805112\n","94.52875399361022\n","94.52875399361022\n","94.55870607028754\n","94.62859424920129\n","94.64856230031948\n","94.6785143769968\n","94.6685303514377\n","94.70846645367412\n","94.72843450479233\n","94.77835463258786\n","94.76837060702876\n","94.80830670926518\n","94.81829073482429\n","94.87819488817891\n","94.83825878594249\n","94.90814696485623\n","94.95806709265176\n","94.95806709265176\n","94.98801916932908\n","95.02795527156549\n","95.08785942492013\n","95.10782747603834\n","95.11781150159744\n","95.15774760383387\n","95.1976837060703\n","95.2076677316294\n","95.2076677316294\n","95.25758785942492\n","95.26757188498402\n","95.29752396166134\n","95.30750798722045\n","95.35742811501598\n","95.3973642172524\n","95.40734824281151\n","95.49720447284345\n","95.52715654952077\n","95.54712460063898\n","95.54712460063898\n","95.6070287539936\n","95.61701277955271\n","95.65694888178913\n","95.72683706070288\n","95.71685303514377\n","95.7567891373802\n","95.7667731629393\n","95.77675718849841\n","95.85662939297124\n","95.84664536741214\n","95.89656549520767\n","95.88658146964856\n","95.89656549520767\n","95.96645367412141\n","95.99640575079871\n","96.03634185303514\n","96.11621405750799\n","96.15615015974441\n","96.14616613418531\n","96.17611821086263\n","96.18610223642173\n","96.24600638977635\n","96.22603833865814\n","96.28594249201278\n","96.25599041533546\n","96.35583067092652\n","96.35583067092652\n","96.36581469648563\n","96.38578274760384\n","96.42571884984025\n","96.42571884984025\n","96.47563897763578\n","96.47563897763578\n","96.47563897763578\n","96.5055910543131\n","96.495607028754\n","96.5055910543131\n","96.55551118210863\n","96.60543130990415\n","96.61541533546325\n","96.61541533546325\n","96.61541533546325\n","96.62539936102236\n","96.65535143769968\n","96.64536741214057\n","96.70527156549521\n","96.73522364217253\n","96.71525559105432\n","96.72523961661342\n","96.77515974440895\n","96.77515974440895\n","96.83506389776358\n","96.85503194888179\n","96.8849840255591\n","96.91493610223642\n","96.92492012779553\n","96.96485623003196\n","96.96485623003196\n","96.98482428115015\n","97.01477635782747\n","97.02476038338658\n","97.03474440894568\n","97.03474440894568\n","97.00479233226837\n","97.04472843450479\n","97.0547124600639\n","97.064696485623\n","97.11461661341853\n","97.12460063897764\n","97.11461661341853\n","97.14456869009585\n","97.14456869009585\n","97.18450479233226\n","97.19448881789137\n","97.254392971246\n","97.2344249201278\n","97.29432907348243\n","97.27436102236422\n","97.254392971246\n","97.30431309904154\n","97.28434504792332\n","97.32428115015975\n","97.31429712460064\n","97.36421725239616\n","97.44408945686901\n","97.47404153354633\n","97.48402555910543\n","97.46405750798722\n","97.49400958466454\n","97.49400958466454\n","97.52396166134186\n","97.52396166134186\n","97.55391373801918\n","97.55391373801918\n","97.6138178913738\n","97.6138178913738\n","97.64376996805112\n","97.64376996805112\n","97.65375399361022\n","97.68370607028754\n","97.66373801916933\n","97.65375399361022\n","97.71365814696486\n","97.71365814696486\n","97.72364217252397\n","97.75359424920129\n","97.77356230031948\n","97.76357827476038\n","97.77356230031948\n","97.81349840255591\n","97.78354632587859\n","97.82348242811501\n","97.85343450479233\n","97.86341853035144\n","97.86341853035144\n","97.87340255591054\n","97.88338658146965\n","97.88338658146965\n","97.90335463258786\n","97.88338658146965\n","97.88338658146965\n","97.90335463258786\n","97.95327476038338\n","97.97324281150159\n","97.9932108626198\n","98.00319488817891\n","98.00319488817891\n","97.9832268370607\n","98.00319488817891\n","98.00319488817891\n","97.9932108626198\n","98.00319488817891\n","98.02316293929712\n","98.04313099041534\n","98.05311501597444\n","98.04313099041534\n","98.04313099041534\n","98.06309904153355\n","98.05311501597444\n","98.06309904153355\n","98.09305111821087\n","98.11301916932908\n","98.12300319488818\n","98.09305111821087\n","98.13298722044729\n","98.1629392971246\n","98.1629392971246\n","98.1629392971246\n","98.1729233226837\n","98.22284345047923\n","98.23282747603834\n","98.24281150159744\n","98.26277955271566\n","98.27276357827476\n","98.27276357827476\n","98.26277955271566\n","98.26277955271566\n","98.29273162939297\n","98.29273162939297\n","98.30271565495208\n","98.3226837060703\n","98.34265175718849\n","98.31269968051119\n","98.3326677316294\n","98.34265175718849\n","98.3326677316294\n","98.3326677316294\n","98.34265175718849\n","98.3326677316294\n","98.3526357827476\n","98.3626198083067\n","98.3626198083067\n","98.39257188498402\n","98.41253993610223\n","98.42252396166134\n","98.42252396166134\n","98.47244408945687\n","98.48242811501598\n","98.49241214057508\n","98.49241214057508\n","98.49241214057508\n","98.5123801916933\n","98.50239616613419\n","98.5123801916933\n","98.53234824281151\n","98.5223642172524\n","98.5523162939297\n","98.5423322683706\n","98.58226837060703\n","98.59225239616613\n","98.60223642172524\n","98.61222044728434\n","98.60223642172524\n","98.63218849840256\n","98.65215654952077\n","98.65215654952077\n","98.66214057507987\n","98.65215654952077\n","98.69209265175719\n","98.69209265175719\n","98.7020766773163\n","98.7120607028754\n","98.72204472843451\n","98.7320287539936\n","98.7320287539936\n","98.7320287539936\n","98.7320287539936\n","98.76198083067092\n","98.77196485623003\n","98.77196485623003\n","98.79193290734824\n","98.78194888178913\n","98.79193290734824\n","98.80191693290735\n","98.81190095846645\n","98.80191693290735\n","98.81190095846645\n","98.82188498402556\n","98.81190095846645\n","98.81190095846645\n","98.83186900958466\n","98.83186900958466\n","98.87180511182109\n","98.87180511182109\n","98.92172523961662\n","98.91174121405751\n","98.91174121405751\n","98.91174121405751\n","98.96166134185303\n","98.95167731629392\n","98.97164536741214\n","98.96166134185303\n","98.99161341853035\n","99.02156549520767\n","99.02156549520767\n","99.03154952076677\n","99.02156549520767\n","99.03154952076677\n","99.05151757188499\n","99.05151757188499\n","99.0714856230032\n","99.0814696485623\n","99.09145367412141\n","99.10143769968052\n","99.09145367412141\n","99.13138977635782\n","99.13138977635782\n","99.14137380191693\n","99.15135782747603\n","99.15135782747603\n","99.15135782747603\n","99.15135782747603\n","99.14137380191693\n","99.16134185303514\n","99.18130990415335\n","99.18130990415335\n","99.18130990415335\n","99.18130990415335\n","99.18130990415335\n","99.19129392971246\n","99.19129392971246\n","99.21126198083067\n","99.20127795527156\n","99.19129392971246\n","99.22124600638978\n","99.23123003194888\n","99.24121405750799\n","99.24121405750799\n","99.24121405750799\n","99.2511980830671\n","99.27116613418531\n","99.2611821086262\n","99.29113418530352\n","99.30111821086263\n","99.31110223642173\n","99.31110223642173\n","99.32108626198082\n","99.31110223642173\n","99.32108626198082\n","99.33107028753993\n","99.34105431309904\n","99.35103833865814\n","99.34105431309904\n","99.35103833865814\n","99.36102236421725\n","99.35103833865814\n","99.37100638977635\n","99.37100638977635\n","99.36102236421725\n","99.38099041533546\n","99.38099041533546\n","99.40095846645367\n","99.40095846645367\n","99.40095846645367\n","99.41094249201278\n","99.41094249201278\n","99.42092651757189\n","99.42092651757189\n","99.43091054313099\n","99.43091054313099\n","99.43091054313099\n","99.43091054313099\n","99.4508785942492\n","99.4408945686901\n","99.4508785942492\n","99.46086261980831\n","99.47084664536742\n","99.47084664536742\n","99.48083067092652\n","99.48083067092652\n","99.49081469648563\n","99.49081469648563\n","99.49081469648563\n","99.50079872204473\n","99.50079872204473\n","99.50079872204473\n","99.50079872204473\n","99.50079872204473\n","99.51078274760384\n","99.50079872204473\n","99.53075079872204\n","99.53075079872204\n","99.54073482428115\n","99.55071884984025\n","99.54073482428115\n","99.54073482428115\n","99.55071884984025\n","99.54073482428115\n","99.56070287539936\n","99.55071884984025\n","99.55071884984025\n","99.55071884984025\n","99.58067092651757\n","99.57068690095846\n","99.58067092651757\n","99.58067092651757\n","99.58067092651757\n","99.58067092651757\n","99.56070287539936\n","99.56070287539936\n","99.58067092651757\n","99.58067092651757\n","99.59065495207668\n","99.60063897763578\n","99.60063897763578\n","99.60063897763578\n","99.60063897763578\n","99.60063897763578\n","99.59065495207668\n","99.60063897763578\n","99.60063897763578\n","99.60063897763578\n","99.60063897763578\n","99.60063897763578\n","99.61062300319489\n","99.620607028754\n","99.620607028754\n","99.620607028754\n","99.6305910543131\n","99.6305910543131\n","99.6305910543131\n","99.620607028754\n","99.6305910543131\n","99.6305910543131\n","99.6305910543131\n","99.6305910543131\n","99.6405750798722\n","99.6405750798722\n","99.65055910543131\n","99.65055910543131\n","99.65055910543131\n","99.66054313099042\n","99.66054313099042\n","99.66054313099042\n","99.66054313099042\n","99.65055910543131\n","99.66054313099042\n","99.66054313099042\n","99.66054313099042\n","99.67052715654953\n","99.66054313099042\n","99.67052715654953\n","99.68051118210863\n","99.68051118210863\n","99.69049520766774\n","99.69049520766774\n","99.68051118210863\n","99.69049520766774\n","99.69049520766774\n","99.69049520766774\n","99.70047923322684\n","99.70047923322684\n","99.70047923322684\n","99.70047923322684\n","99.70047923322684\n","99.70047923322684\n","99.70047923322684\n","99.70047923322684\n","99.70047923322684\n","99.70047923322684\n","99.70047923322684\n","99.70047923322684\n","99.70047923322684\n","99.70047923322684\n","99.70047923322684\n","99.70047923322684\n","99.70047923322684\n","99.70047923322684\n","99.70047923322684\n","99.70047923322684\n","99.70047923322684\n","99.70047923322684\n","99.70047923322684\n","99.69049520766774\n","99.70047923322684\n","99.71046325878594\n","99.71046325878594\n","99.71046325878594\n","99.71046325878594\n","99.70047923322684\n","99.71046325878594\n","99.71046325878594\n","99.71046325878594\n","99.71046325878594\n","99.71046325878594\n","99.71046325878594\n","99.71046325878594\n","99.71046325878594\n","99.71046325878594\n","99.71046325878594\n","99.71046325878594\n","99.71046325878594\n","99.71046325878594\n","99.71046325878594\n","99.71046325878594\n","99.71046325878594\n","99.71046325878594\n","99.72044728434504\n","99.72044728434504\n","99.72044728434504\n","99.72044728434504\n","99.72044728434504\n","99.72044728434504\n","99.71046325878594\n","99.72044728434504\n","99.71046325878594\n","99.72044728434504\n","99.72044728434504\n","99.72044728434504\n","99.73043130990415\n","99.73043130990415\n","99.73043130990415\n","99.73043130990415\n","99.73043130990415\n","99.74041533546325\n","99.74041533546325\n","99.74041533546325\n","99.74041533546325\n","99.74041533546325\n","99.74041533546325\n","99.74041533546325\n","99.74041533546325\n","99.75039936102236\n","99.75039936102236\n","99.75039936102236\n","99.75039936102236\n","99.75039936102236\n","99.75039936102236\n","99.75039936102236\n","--------\n","final accuracy\n","77.81\n"]}],"source":["print(\"Number of full training batch iterations:\",len(batch_train_accs))\n","print(\"Accuracies per training batch:\")\n","for i in batch_train_accs:\n","  print(i)\n","y_test_pred1 = model1.predict(x_test)\n","print(\"--------\")\n","print(\"final accuracy\")\n","print(evaluate_acc(y_test_pred1, y_test))"]},{"cell_type":"code","source":["fig, ax = plt.subplots()\n","ax.scatter(range(len(batch_train_accs)), batch_train_accs, c='r', marker='x')\n","ax.legend()\n","ax.grid(True)\n","plt.ylabel('accuracy')\n","plt.xlabel('epoch')\n","plt.show()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":297},"id":"G8nPTn6wvRoP","executionInfo":{"status":"ok","timestamp":1649099747112,"user_tz":240,"elapsed":237,"user":{"displayName":"Seraphin Bassas","userId":"17841861031575039122"}},"outputId":"f3e9b319-0a6d-49e7-85a6-ea9c88fc987c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["No handles with labels found to put in legend.\n"]},{"output_type":"display_data","data":{"text/plain":["<Figure size 432x288 with 1 Axes>"],"image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"code","source":["print(evaluate_acc(y_test_pred1, y_test))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1uRpgXt56KO8","executionInfo":{"status":"ok","timestamp":1649099470669,"user_tz":240,"elapsed":10,"user":{"displayName":"Seraphin Bassas","userId":"17841861031575039122"}},"outputId":"7b958603-92c9-4453-bb51-1b4a90641008"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["77.81\n"]}]}],"metadata":{"colab":{"collapsed_sections":[],"name":"3LayerMLPUnormalized.ipynb","provenance":[{"file_id":"1AE_3CunEj0UW8eQz2bH_Vh7m5QBrldPI","timestamp":1648694691980}]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.6"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}